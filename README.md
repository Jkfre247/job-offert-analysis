# Data Science Junior Job Market Analysis
The aim of this program was to analyze and visualize the job market for Data Science junior positions in Poland. The data for analysis was collected from four regions - Poland, USA, UK, and German-speaking countries (Germany, Austria, Switzerland) using the Selenium library. The visualization is specific to Poland but is supported by international databases in the categories of required technologies and the number of proposals for data scientists compared to data-related jobs. The collected data underwent cleaning to remove empty and duplicate records (job descriptions).

The program includes preliminary data visualization, and Azure Translator was used to identify the language of the job descriptions and translate them into English. Additionally, NLP techniques were employed by using NER from the Azure environment to analyze job descriptions. The data was prepared for visualization using Tableau software.
# Result of my analysis
The result of my analysis shows that of all the job adverts for 'Data Scientist' with a 'Junior' level requirement, 83% were actual Data Scientist roles in Data jobs (many of the offers are from different fields). The majority of these job adverts were in the US. Companies prefer to hire full-time and then hire on a contract basis. Warsaw has the most job ads for this role in Poland. The most sought-after technologies are:

Programming languages: Python, R, Java
Big data: SQL, Spark, Hadoop
Cloud services: AWS, Azure, Google
Frameworks: TensorFlow, Scikit-learn, Pandas
BI: Tableau, Power BI.
![Tableau result](https://github.com/Jkfre247/job-offert-analysis/blob/main/Data%20Science%20Analysis.png)

# Data Collecting
The data was collected from LinkedIn by searching for the phrase "Data Scientist" and selecting the role as a junior in the chosen country. The program undergoes several security measures, from logging in to accepting cookie files. Upon entering the job postings page, the program encounters a page divided into two sections. The left section contains the list of job postings, and the right section displays specific job offer. The program passes through security measures(The page loads only a portion of the job postings, so the program scrolls down a certain element to obtain all job postings.) to display all job postings.  The program then downloads the appropriate job posting blocks. When a specific block is clicked and a specific offert is displayed, the program then downloads information such as the company name, job position, location, job description, etc.

To save time, the program employs a function that checks if the job posting is within our interest. If not, the job posting is skipped, which significantly streamlines the data collection process. The program also includes basic security measures to avoid disrupting the LinkedIn service and to prevent detection. The program uses the function time.sleep, which not only allows the page to load but also provides a small security measure. After going through all available job postings (up to 1000), the program saves the data in a CSV file.
# Data Cleaning
After the data collection, the program reads the collected data and creates three new columns - country, region, and city using location column. Program fixed broken records that have incorrect data, such as Great Britain, Londyn, Londyn to Great Britain, England, Londyn. The program removes duplicates by looking at the "Description" column. While my initial idea was to have only one job posting per company, I realized that companies could have multiple departments, such as one focused on NLP and another focused on a different area. Therefore, I decided to remove duplicates based on job descriptions to avoid repeated job postings. The program then removes empty rows.

Some job postings from the USA included salary information, which was not present in job postings from other regions. This caused issues with the "Level" and "Type" columns, so the program repaired these cells. Additionally, the program fixed several location-related records.

The programme created new databases into four categories - Data Scientist, Data Analyst, AI and ML - and again removes duplicates to include the same job advert appearing in multiple countries. The cleaned data is then saved.
# Preliminary Data Visualization
In the Preliminary Data Visualization section, I provided an initial visualization of the collected data. I compared the number of job postings for Data Science to those for related jobs (AI, ML, Data Analyst). I also provided a visualization of the number of job postings in each category, broken down by country.

I also created visualizations showing the distribution of job postings for Data Science across different regions (voivodeships) and cities in Poland and the last bar chart is the type of employment - full-time, part-time or contracted across the data scientist data frame.
# Language Identification and Translation
In the Language Identification and Translation section, I utilized the Azure environment. I used Azure Translator to translate all job descriptions to English. While it is possible to use multilingual databases, I do not have an advanced knowledge of the German language, and Polish is not supported by Azure Cognitive Services. The program first detects the language, and then translates it to English after detecting the language.
# NLP Analysis
In the NLP Analysis section, I started by clearing the data of any unnecessary characters. However, in preparing for the visualisation, I realised that I should have left some useful characters, such as '-', to avoid separating phrases such as 'scikit-learn' or 'c#'. For future reference, I will review whether certain characters might be useful.

I then removed the excess white spaces, removed the stop words and performed the lemmatisation process on the words. For the lemmatisation process to work correctly, I had to write a function that detects parts of speech to ensure that the lemmatisation process is compiled correctly. In some cases, such as with technology-related proper nouns such as 'pandas' and 'Keras', lemmatisation was not the best idea as the words were truncated from the 's' endings which could lead to incorrect analysis. However, this did not negatively affect the final outcome, but I could add exceptions to the program to avoid lemmatizing such words.

After testing various possibilities, I decided that using Azure's NER algorithm to recognise categories would be preferable to Keyphrases for my purposes. I encountered a problem with the size of the uploads (the list could not exceed five items and the descriptions could not exceed a certain size), so I implemented a function to split the text into smaller chunks. After running the function, I got a new database with 'Entity' and 'Category' columns, which were sufficient for further analysis. Unfortunately, after checking the various options, I was surprised by the low number of occurrences of some languages such as Python and R. I am not sure why, but NER did not identify a significant number of these words. I tried using one description per NER function call, but this only slightly increased the number of identified instances. In the next section I tested what wordcloud would look like for Keyphrases. Further use of the results from the NER is in the next section
# Data Preparation for Visualization
In part six, the focus was mainly on preparing the data for visualization in Tableau, which is visible at the beginning of the readme file. Initially, I examined how some charts would be presented. The word cloud using the Entity column (obtained through NER) from the "Skill" category showed key words very well. Unfortunately, there was no "Technology" category in NER, but I noticed that all technologies from big data and frameworks to programming languages were in the "Product" category. To extract technologies from the rest of the words, I used the help of a well-developed text analyzer, ChatGPT, to extract the names from ordinary words (I obtained all names by using unique() in "Category"=="Product"). ChatGPT correctly classified everything. Unfortunately, after analyzing the number of technological words, I started to check if words like Python or others appeared so infrequently. However, as I mentioned earlier, some words were cut off. Nonetheless, the NER algorithm helped me in analyzing the key words in the "Skill" category and in identifying all technological names with the help of ChatGPT. To search for technological words, I used the re library, which searched all translated descriptions for words in the created categories of "Bigdata", "BI", "cloud_services", "programming_language", and "frameworks" created using NER. Then, I recreated all the charts I deemed appropriate using Tableau, and the final effect of my hard work was achieved.
